"""
tiga.py — TIGA Hunt CLI entrypoint.

Subcommands:
  init         Generate default config.yaml in work_dir
  discover     List files that would be indexed (dry run)
  extract      Test text extraction on a single file
  embed        Test Ollama embedding on a query string
  index        Index archive files (incremental by default)
  rebuild      Force full re-index
  query        Search the archive from terminal
  status       Show index stats
  eval         Run search quality evaluation
  serve        Start the FastAPI LAN server
  conventions  Detect/show/override project folder naming conventions
  einstein     Phase 2 (stub)
  scan         Windirstat-style file type / size scan of a project folder
  scrape-woha  Crawl woha.net and seed project_cards with project metadata
  schedule     Manage the time-of-day resource scheduler (day/night mode)
"""

from __future__ import annotations

import argparse
import json
import sys
import time


# ---------------------------------------------------------------------------
# Default config template (written by `init`)
# ---------------------------------------------------------------------------

_CONFIG_TEMPLATE = """\
# TIGA Hunt — Configuration
# Generated by: python tiga.py init
# Edit index_roots to point at your archive directories.

work_dir: ./tiga_work

# Directories to scan and index
index_roots:
  - "Z:/186 - Tianmu"
  - "Z:/242 - World Expo Pavilion"
  - "Z:/232 - SIT Campus"

include_globs:
  - '**/*'
exclude_globs:
  - '**/.git/**'
  - '**/~$*'
  - '**/*.tmp'
  - '**/node_modules/**'
  - '**/__pycache__/**'

max_file_mb: 2048

lane_rules:
  text_extractable_exts: [.pdf, .docx, .pptx, .txt, .md, .doc]
  metadata_only_exts: [.dwg, .rvt, .ifc, .skp, .3dm, .jpg, .jpeg, .png, .mp4, .mov, .avi]

ollama:
  base_url: http://localhost:11434
  embed_model: nomic-embed-text
  chat_model: mistral
  timeout_seconds: 30
  gpu_layers: -1
  num_ctx: 4096
  embed_batch_size: 32
  embed_batch_sleep_s: 0.1

server:
  host: 0.0.0.0
  port: 7860
  workers: 2

ui:
  port: 8501
  title: TIGA Hunt
  max_session_history: 20

project_inference:
  enable: true
  confidence_threshold_unknown: 0.5
  patterns:
    - name: year-code
      regex: '^(\\d{3,4})[-_ ]'
      weight: 0.8
  keyword_boosts:
    - {keyword: brief,      weight: 0.2}
    - {keyword: tender,     weight: 0.2}
    - {keyword: submission, weight: 0.2}
    - {keyword: SD,         weight: 0.1}
    - {keyword: DD,         weight: 0.1}

typology_inference:
  confidence_threshold_unknown: 0.5
  keyword_map:
    healthcare:   [hospital, clinic, healthcare, medical]
    education:    [school, campus, university, polytechnic, sit]
    sports:       [sports, stadium, arena, aquatic]
    residential:  [residential, housing, hdb, apartment]
    commercial:   [office, commercial, retail, hotel]
    cultural:     [museum, gallery, library, theatre, pavilion, expo]

retrieval:
  top_k_default: 5
  hybrid_weight_bm25: 0.4
  hybrid_weight_vector: 0.6
  # Cross-encoder reranker — dramatically improves result precision.
  # Install: pip install sentence-transformers
  # Model downloads ~80 MB on first use, runs on CPU in ~15-20 ms.
  reranker_enabled: false
  reranker_model: cross-encoder/ms-marco-MiniLM-L-6-v2
  reranker_top_k: 20  # rerank top-N candidates before trimming to top_k_default

ocr:
  enabled: false
  tesseract_cmd: tesseract

einstein:
  enable: false
  base_model: mistral
  adapter_path: ./tiga_work/einstein/adapter

# Pipeline parallelism — set extract_workers to your CPU core count.
# On an i9 with 8+ cores, 6-8 workers speeds up PDF/DOCX extraction dramatically.
pipeline:
  extract_workers: 4
  # Fingerprint strategy for change detection:
  #   full     — SHA256 of entire file (slow, most accurate)
  #   sampled  — SHA256 of head+tail 64 KB (fast, 99.9% accurate)
  #   metadata — size + mtime only (fastest; fine for local/NAS filesystems)
  fingerprint_strategy: sampled

# Time-of-day resource scheduler — balances LAN portal serving vs. background indexing.
# Day mode (day_start_hour to night_start_hour): portal-optimised, low background load.
# Night mode: saturate GPU+CPU for maximum indexing throughput.
scheduler:
  day_start_hour: 6        # 06:00 → switch to day mode
  night_start_hour: 22     # 22:00 → switch to night mode
  # Day: leave VRAM for the chat model; minimal CPU background
  day_embed_batch_size: 16
  day_extract_workers: 2
  day_run_indexing: false
  # Night: push the 4090 and i9 hard
  night_embed_batch_size: 256
  night_extract_workers: 8
  night_run_indexing: true
"""


# ---------------------------------------------------------------------------
# Eval fixture template
# ---------------------------------------------------------------------------

_FIXTURE_TEMPLATE = """\
# TIGA Hunt — eval fixture
# Edit these entries to match your POC archive.
# Run: python tiga.py eval
#
# expected_paths: suffix-match against indexed file_path (partial paths OK)
# expected_project: optional label (not used in scoring yet)

- query: "hospital brief 2019"
  expected_paths:
    - 2019_HOSP/brief.pdf
  expected_project: "2019_HOSP"

- query: "school competition submission"
  expected_paths:
    - Competitions/submission.pdf

- query: "site analysis residential"
  expected_paths:
    - site_analysis.pdf
"""


# ---------------------------------------------------------------------------
# Commands
# ---------------------------------------------------------------------------

def cmd_init(args: argparse.Namespace) -> None:
    """Generate default config.yaml and eval fixture in work_dir."""
    import os
    from pathlib import Path

    work_dir_env = os.environ.get("TIGA_WORK_DIR")
    work_dir = Path(work_dir_env).resolve() if work_dir_env else (
        Path(__file__).resolve().parent / "tiga_work"
    )
    config_file = work_dir / "config.yaml"

    work_dir.mkdir(parents=True, exist_ok=True)

    if config_file.exists() and not args.force:
        print(f"Config already exists: {config_file}")
        print("Use --force to overwrite.")
        return

    config_file.write_text(_CONFIG_TEMPLATE, encoding="utf-8")
    print(f"Created: {config_file}")

    # Eval fixture (always written; non-destructive)
    fixtures_dir = work_dir / "fixtures"
    fixtures_dir.mkdir(parents=True, exist_ok=True)
    fixture_file = fixtures_dir / "eval_queries.yaml"
    if not fixture_file.exists():
        fixture_file.write_text(_FIXTURE_TEMPLATE, encoding="utf-8")
        print(f"Created: {fixture_file}")

    print("\nNext steps:")
    print("  1. Edit tiga_work/config.yaml — set your index_roots")
    print("  2. Edit tiga_work/fixtures/eval_queries.yaml — add test queries")
    print("  3. Install Ollama from https://ollama.com")
    print("  4. Run: ollama pull nomic-embed-text && ollama pull mistral")
    print("  5. Run: python tiga.py index")
    print("  6. Run: python tiga.py serve   (in one terminal)")
    print("          python tiga.py ui      (in another)")


def cmd_discover(args: argparse.Namespace) -> None:
    from collections import Counter
    from core.discover import discover

    print("Scanning index_roots…")
    files = discover()

    # Filter by project if requested
    if getattr(args, "project", None):
        project_filter = args.project.lower()
        files = [f for f in files if project_filter in str(f).lower()]
        print(f"Filtered to project '{args.project}': {len(files)} files")

    ext_counts: Counter[str] = Counter(f.suffix.lower() for f in files)
    print(f"\nFound {len(files)} files\n")
    print(f"{'Extension':<12} {'Count':>6}")
    print("-" * 20)
    for ext, count in sorted(ext_counts.items(), key=lambda x: -x[1]):
        print(f"{ext:<12} {count:>6}")

    if getattr(args, "parsed", False):
        from config import cfg
        from core.path_parser import parse_file_path, load_semantics

        sem_path = cfg.work_dir / "folder_semantics.yml"
        semantics = load_semantics(sem_path)

        print(f"\n{'File':<50} {'Stage':<14} {'Discipline':<14} {'Doc Type':<22} {'Rev':<5} {'Date'}")
        print("-" * 125)
        unknown_stage = 0
        unknown_disc = 0

        for f in files[:200]:  # cap at 200 to keep output readable
            for root in cfg.index_roots:
                try:
                    parsed = parse_file_path(str(f), str(root), semantics)
                    name = f.name[:48]
                    stage = parsed.get("folder_stage", "?")
                    disc = parsed.get("discipline", "?")
                    doc = parsed.get("doc_type", "?")
                    rev = str(parsed.get("revision", "")) or ""
                    dt = parsed.get("file_date", "") or ""
                    flag = " ⚠" if stage == "unknown" or disc == "unknown" else ""
                    print(f"  {name:<50} {stage:<14} {disc:<14} {doc:<22} {rev:<5} {dt}{flag}")
                    if stage == "unknown":
                        unknown_stage += 1
                    if disc == "unknown":
                        unknown_disc += 1
                    break
                except ValueError:
                    continue

        if len(files) > 200:
            print(f"\n  … (showing 200 of {len(files)} files)")
        print(f"\n  Unknown stage: {unknown_stage} | Unknown discipline: {unknown_disc}")
        return

    if getattr(args, "list", False):
        print("\nFiles:")
        for f in files:
            print(f"  {f}")


def cmd_extract(args: argparse.Namespace) -> None:
    from pathlib import Path
    from core.extract import extract

    path = Path(args.file)
    if not path.exists():
        print(f"File not found: {path}", file=sys.stderr)
        sys.exit(1)

    print(f"Extracting: {path}")
    text, surrogate = extract(path)
    print(f"\n--- Surrogate ---\n{surrogate}")
    if text:
        preview = text[:1000]
        print(f"\n--- Text preview (first 1000 chars) ---\n{preview}")
        if len(text) > 1000:
            print(f"… [{len(text) - 1000} more chars]")
    else:
        print("\n[No text extracted — metadata-only file]")


def cmd_embed(args: argparse.Namespace) -> None:
    from config import ollama_available, cfg
    from core.vectors import embed_texts

    query = " ".join(args.query)
    print(f"Query: {query!r}")

    if not ollama_available(cfg.ollama_base_url):
        print("Ollama is not reachable. Is it running?", file=sys.stderr)
        sys.exit(1)

    t0 = time.perf_counter()
    embeddings = embed_texts([query])
    elapsed = round((time.perf_counter() - t0) * 1000, 1)
    vec = embeddings[0]
    print(f"Embedding dimensions: {len(vec)}")
    print(f"First 8 values: {[round(v, 4) for v in vec[:8]]}")
    print(f"Elapsed: {elapsed} ms")


def cmd_index(args: argparse.Namespace) -> None:
    from config import cfg
    from core.db import get_connection
    from core.index import run_full_pipeline

    cfg.ensure_dirs()
    conn = get_connection(cfg.get_db_path())
    print("Starting incremental index (discover → extract → embed → FTS)…")
    t0 = time.perf_counter()
    try:
        stats = run_full_pipeline(conn, cfg_obj=cfg)
    finally:
        conn.close()
    elapsed = round(time.perf_counter() - t0, 1)
    print(
        f"\nDone in {elapsed}s — "
        f"discovered: {stats.get('discovered', 0)} new files | "
        f"extracted: {stats.get('files_extracted', 0)} files "
        f"({stats.get('chunks_new', 0)} chunks) | "
        f"embedded: {stats.get('files_embedded', 0)} files, "
        f"{stats.get('chunks_embedded', 0)} chunks "
        f"({stats.get('chunks_skipped', 0)} skipped) | "
        f"indexed: {stats.get('files_indexed', 0)} files"
    )


def cmd_rebuild(args: argparse.Namespace) -> None:
    from config import cfg
    from core.db import get_connection
    from core.index import run_rebuild

    cfg.ensure_dirs()
    conn = get_connection(cfg.get_db_path())
    print("Rebuilding index from scratch (FTS drop + LanceDB reset)…")
    t0 = time.perf_counter()
    try:
        stats = run_rebuild(conn, cfg_obj=cfg)
    finally:
        conn.close()
    elapsed = round(time.perf_counter() - t0, 1)
    print(
        f"\nRebuild complete in {elapsed}s — "
        f"embedded: {stats.get('files_embedded', 0)} files, "
        f"{stats.get('chunks_embedded', 0)} chunks | "
        f"indexed: {stats.get('files_indexed', 0)} files"
    )


def cmd_query(args: argparse.Namespace) -> None:
    from core.query import search
    from core.compose import compose
    from config import cfg

    query = " ".join(args.query)
    if not query:
        print("Usage: tiga.py query <your question>")
        sys.exit(1)

    print(f"\nSearching: {query!r}\n")
    t0 = time.perf_counter()
    results = search(query, top_k=args.top_k)
    answer = compose(query, results)
    elapsed = round((time.perf_counter() - t0) * 1000, 1)

    print(answer)
    print(f"\n--- {elapsed} ms | {len(results)} results ---")
    for i, r in enumerate(results, 1):
        print(
            f"  [{i}] {r.get('file_name', r.get('rel_path', ''))}  "
            f"project={r.get('project_id', r.get('project', 'Unknown'))}  "
            f"typology={r.get('typology', 'Unknown')}  "
            f"score={r.get('final_score', r.get('combined_score', 0.0)):.3f}  "
            f"cite={r.get('citation', '')}"
        )


def cmd_status(_args: argparse.Namespace) -> None:
    from config import cfg
    from core.db import get_connection, get_stats

    conn = get_connection(cfg.get_db_path())
    stats = get_stats(conn)
    conn.close()
    print(json.dumps(stats, indent=2))
    print(f"index_roots: {[str(d) for d in cfg.index_roots]}")


def cmd_eval(args: argparse.Namespace) -> None:
    if getattr(args, "review_failures", False):
        _cmd_review_failures()
        return

    if getattr(args, "routing", False):
        from core.eval import run_routing_eval
        code = run_routing_eval(verbose=True)
        sys.exit(code)
        return

    if getattr(args, "stress", False):
        from core.eval import run_stress_eval
        code = run_stress_eval(project=getattr(args, "project", None))
        sys.exit(code)
        return

    from core.eval import run_eval

    queries = args.queries if args.queries else None
    code = run_eval(queries=queries, top_k=args.top_k)
    sys.exit(code)


def _cmd_review_failures() -> None:
    """Interactively review low-confidence queries and create corrections."""
    from config import cfg

    log_file = cfg.work_dir / "logs" / "low_confidence.log"
    if not log_file.exists():
        print("No low-confidence queries logged yet.")
        return

    import json as _json
    entries = []
    with open(log_file, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    entries.append(_json.loads(line))
                except Exception:
                    pass

    recent = entries[-20:]
    if not recent:
        print("No low-confidence queries found.")
        return

    print(f"\nReviewing last {len(recent)} low-confidence queries")
    print("=" * 60)
    corrected = 0

    for entry in recent:
        query = entry.get("query", "")
        confidence = entry.get("confidence", 0)
        mode = entry.get("mode", "?")
        ts = entry.get("timestamp", "?")
        print(f"\n[{ts[:16]}] {query!r}")
        print(f"  Mode: {mode}, Confidence: {confidence:.2f}")
        try:
            answer = input("  Was this answer correct? (y/n/skip) ").strip().lower()
        except (KeyboardInterrupt, EOFError):
            print("\nAborted.")
            return

        if answer == "n":
            try:
                project = input("  Project code: ").strip()
                field = input("  Field that was wrong (or 'answer'): ").strip()
                correct_val = input("  Correct value: ").strip()
            except (KeyboardInterrupt, EOFError):
                print("\nAborted.")
                return

            if project and field and correct_val:
                from core.corrections import save_correction
                save_correction(
                    project_code=project,
                    field=field,
                    correct_value=correct_val,
                    wrong_value=None,
                    source_path=None,
                )
                print(f"  Saved correction: {project}.{field} = {correct_val!r}")
                corrected += 1
        elif answer == "skip":
            continue

    print(f"\nDone. {corrected} correction(s) saved.")


def cmd_serve(_args: argparse.Namespace) -> None:
    from config import cfg
    import uvicorn

    print(f"Starting TIGA Hunt server at http://{cfg.server_host}:{cfg.server_port}")
    uvicorn.run(
        "server:app",
        host=cfg.server_host,
        port=cfg.server_port,
        workers=cfg.server_workers,
        reload=False,
    )


def cmd_ui(_args: argparse.Namespace) -> None:
    """Start the Streamlit admin panel (port = server_port + 1)."""
    import socket
    import subprocess
    from config import cfg

    admin_port = cfg.server_port + 1
    try:
        local_ip = socket.gethostbyname(socket.gethostname())
    except Exception:
        local_ip = "127.0.0.1"
    print(f"TIGA Admin: http://{local_ip}:{admin_port}")
    subprocess.run(
        [sys.executable, "-m", "streamlit", "run", "app.py",
         "--server.port", str(admin_port),
         "--server.address", "0.0.0.0",
         "--server.headless", "true"],
        check=True,
    )


def cmd_card(args: argparse.Namespace) -> None:
    """View, scrape, or interactively edit a project data card."""
    from config import cfg
    from core.db import get_connection
    from core.project_card import (
        get_project_card, upsert_project_card,
        get_missing_fields, scrape_woha_project,
    )

    cfg.ensure_dirs()
    conn = get_connection(cfg.get_db_path())
    code = args.project_code

    try:
        # --show: just print the card
        if args.show:
            card = get_project_card(code, conn=conn)
            if card is None:
                print(f"No project card found for '{code}'.")
                print(f"Create one with: python tiga.py card {code}")
                return
            _print_card(card)
            return

        # --woha-url: scrape first, then interactive review
        if args.woha_url:
            print(f"Scraping {args.woha_url} …")
            scraped = scrape_woha_project(args.woha_url)
            scraped["project_code"] = code
            if scraped:
                upsert_project_card(scraped, conn=conn)
                print(f"Scraped {len(scraped) - 2} fields from WOHA.")
            else:
                print("Warning: scrape returned no data. Check the URL.")

        # Interactive edit
        card = get_project_card(code, conn=conn) or {"project_code": code}
        ds = card.get("data_sources") or {}
        print(f"\nProject card: {code}")
        print("=" * 50)
        print("Press Enter to keep current value. Type new value to update.")
        print("Type 'skip' to skip a field.\n")

        _EDITABLE_FIELDS = [
            ("name", "Project name"),
            ("typology_primary", "Primary typology (residential/hospitality/commercial/civic)"),
            ("typology_secondary", "Secondary typology (optional)"),
            ("location", "Location (city, country)"),
            ("client", "Client / Developer"),
            ("stage", "Stage (Design/Tender/Construction/Completed)"),
            ("gfa_sqm", "GFA (sqm)"),
            ("site_area_sqm", "Site area (sqm)"),
            ("storeys_above", "Storeys above grade"),
            ("storeys_below", "Storeys below grade"),
            ("units", "Residential units (leave blank if N/A)"),
            ("keys", "Hotel keys (leave blank if N/A)"),
            ("architect", "Lead architect name"),
            ("pm_job_captain", "PM / Job captain"),
            ("contract_value", "Contract value (or 'confidential')"),
            ("woha_url", "WOHA website URL"),
        ]

        updates: dict = {"project_code": code}
        ds_updates: dict = {}

        for field_key, label in _EDITABLE_FIELDS:
            current = card.get(field_key)
            src = ds.get(field_key, {}).get("source", "unset")
            src_tag = f" [{src}]" if src != "unset" else " [not set]"
            current_display = str(current) if current is not None else "(not set)"
            try:
                user_input = input(f"{label}{src_tag}\n  Current: {current_display}\n  New: ").strip()
            except (KeyboardInterrupt, EOFError):
                print("\nAborted.")
                conn.close()
                return

            if user_input.lower() == "skip" or user_input == "":
                continue

            # Type coercion for numeric fields
            if field_key in ("gfa_sqm", "site_area_sqm"):
                try:
                    user_input = float(user_input.replace(",", ""))
                except ValueError:
                    print(f"  Invalid number, skipping {field_key}")
                    continue
            elif field_key in ("storeys_above", "storeys_below", "units", "keys"):
                try:
                    user_input = int(user_input.replace(",", ""))
                except ValueError:
                    print(f"  Invalid integer, skipping {field_key}")
                    continue

            updates[field_key] = user_input
            ds_updates[field_key] = {"value": user_input, "source": "manual", "confidence": 1.0}

        if len(updates) > 1:  # more than just project_code
            updates["_source"] = "manual"
            updates["_confidence"] = 1.0
            updates["data_sources"] = ds_updates
            upsert_project_card(updates, conn=conn)
            print(f"\nSaved {len(updates) - 3} field(s) to project card.")
        else:
            print("\nNo changes made.")

        # Show missing fields
        missing = get_missing_fields(code, conn=conn)
        if missing:
            print(f"\nStill missing: {', '.join(missing)}")

    finally:
        conn.close()


def _print_card(card: dict) -> None:
    """Pretty-print a project card."""
    import json as _json
    ds = card.get("data_sources") or {}
    print(f"\n{'=' * 60}")
    print(f"  Project Card: {card.get('project_code', '?')}")
    print(f"{'=' * 60}")

    _DISPLAY_FIELDS = [
        ("name", "Name"), ("typology_primary", "Typology"), ("typology_secondary", "Sub-typology"),
        ("location", "Location"), ("client", "Client"), ("stage", "Stage"),
        ("gfa_sqm", "GFA (sqm)"), ("site_area_sqm", "Site area (sqm)"),
        ("storeys_above", "Storeys above"), ("storeys_below", "Storeys below"),
        ("units", "Units"), ("keys", "Keys"), ("beds", "Beds"),
        ("architect", "Architect"), ("pm_job_captain", "PM/Job Captain"),
        ("contract_value", "Contract value"),
        ("concept_summary", "Concept"),
        ("root_path", "Root path"), ("woha_url", "WOHA URL"),
        ("awards", "Awards"),
    ]
    for key, label in _DISPLAY_FIELDS:
        val = card.get(key)
        if val is None:
            continue
        src = ds.get(key, {}).get("source", "?") if ds else "?"
        if isinstance(val, (list, dict)):
            val_str = _json.dumps(val, ensure_ascii=False)[:120]
        else:
            val_str = str(val)[:120]
        print(f"  {label:<20} {val_str}  [{src}]")

    milestones = card.get("milestone_dates")
    if isinstance(milestones, dict) and milestones:
        print(f"  {'Milestones':<20}")
        for k, v in milestones.items():
            print(f"    {k}: {v}")
    print()


def cmd_correct(args: argparse.Namespace) -> None:
    """Save a manual field correction."""
    from core.corrections import save_correction

    save_correction(
        project_code=args.project,
        field=args.field,
        correct_value=args.value,
    )
    print(f"Saved: {args.project}.{args.field} = {args.value!r}")


def cmd_cards(args: argparse.Namespace) -> None:
    """List project cards, optionally showing only those with missing data."""
    from config import cfg
    from core.db import get_connection
    from core.project_card import list_project_cards, get_missing_fields

    cfg.ensure_dirs()
    conn = get_connection(cfg.get_db_path())
    try:
        cards = list_project_cards(conn=conn)
        if not cards:
            print("No project cards found. Create one with: python tiga.py card <code>")
            return

        for card in cards:
            code = card.get("project_code", "?")
            name = card.get("name") or "(name not set)"
            stage = card.get("stage") or "?"
            typology = card.get("typology_primary") or "?"

            if args.missing:
                missing = get_missing_fields(code, conn=conn)
                if missing:
                    print(f"  {code:<8} {name:<40} {typology:<16} {stage:<14}")
                    print(f"           Missing: {', '.join(missing)}")
            else:
                gfa = f"{card.get('gfa_sqm'):,.0f}sqm" if card.get("gfa_sqm") else "?"
                print(f"  {code:<8} {name:<40} {typology:<16} {gfa:<12} {stage}")
    finally:
        conn.close()


def cmd_route(args: argparse.Namespace) -> None:
    """Show how the router would classify a query."""
    from config import cfg
    from core.db import get_connection
    from core.router import get_router

    query = " ".join(args.query)
    if not query:
        print("Usage: tiga.py route \"<query>\"")
        sys.exit(1)

    cfg.ensure_dirs()
    conn = get_connection(cfg.get_db_path())
    try:
        router = get_router()
        router.load_project_codes(conn)
        result = router.classify(query)
    finally:
        conn.close()

    print(f"\nQuery:      {query!r}")
    print(f"Mode:       {result.mode}")
    print(f"Confidence: {result.confidence:.2f}")
    print(f"Project:    {result.project_code or '(none detected)'}")
    print(f"Filters:    {result.filters or '(none)'}")
    print(f"Reason:     {result.reason}")
    if result.secondary_mode:
        print(f"Secondary:  {result.secondary_mode} (ambiguous — confidence < 0.7)")


def cmd_semantics(args: argparse.Namespace) -> None:
    """Audit folder name matching against folder_semantics.yml."""
    from collections import Counter
    from config import cfg
    from core.db import get_connection
    from core.path_parser import load_semantics, match_stage

    cfg.ensure_dirs()
    conn = get_connection(cfg.get_db_path())
    sem_path = cfg.work_dir / "folder_semantics.yml"
    semantics = load_semantics(sem_path)

    try:
        if args.project:
            where = "WHERE project_id LIKE ? OR project_code = ?"
            params = [f"%{args.project}%", args.project]
        else:
            where = ""
            params = []

        rows = conn.execute(
            f"SELECT file_path FROM files {where} LIMIT 50000",
            params,
        ).fetchall()
    finally:
        conn.close()

    from pathlib import Path as _P
    all_folders: list[str] = []
    for row in rows:
        p = _P(row["file_path"])
        for root in cfg.index_roots:
            try:
                rel = p.relative_to(root)
                all_folders.extend(rel.parts[:-1])  # exclude filename
                break
            except ValueError:
                continue

    unmatched: Counter = Counter()
    matched: Counter = Counter()

    for folder in all_folders:
        stage = match_stage(folder, semantics)
        if stage:
            matched[f"{folder} → {stage}"] += 1
        else:
            unmatched[folder] += 1

    scope = f"project {args.project}" if args.project else "all projects"
    print(f"\nFolder semantics audit — {scope}")
    print(f"Scanned {len(all_folders)} folder segments from {len(rows)} files\n")

    if matched:
        print(f"Matched ({len(matched)} unique):")
        for k, cnt in matched.most_common(20):
            print(f"  {cnt:>5}×  {k}")

    print(f"\nUnmatched ({len(unmatched)} unique — add to folder_semantics.yml):")
    for folder, cnt in unmatched.most_common(40):
        print(f"  {cnt:>5}×  {folder!r}")

    print(f"\nSemantics file: {sem_path}")


def cmd_synonyms(args: argparse.Namespace) -> None:
    """Audit or extend query_synonyms.yml."""
    from config import cfg
    from core.router import load_synonyms, expand_query as _expand

    syn_path = cfg.work_dir / "query_synonyms.yml"

    if args.add and args.concept:
        # Quick-add a phrase to an existing concept
        phrase = args.add.lower().strip()
        concept = args.concept.lower().strip()
        synonyms = load_synonyms(syn_path)

        if concept not in synonyms:
            print(f"Unknown concept '{concept}'. Known: {', '.join(sorted(synonyms.keys()))}")
            sys.exit(1)

        existing = [str(s).lower() for s in synonyms[concept].get("synonyms", [])]
        if phrase in existing:
            print(f"'{phrase}' is already in concept '{concept}'.")
            return

        # Append to YAML file
        import yaml as _yaml
        with open(syn_path, "r", encoding="utf-8") as f:
            raw = f.read()

        # Find the synonyms block for this concept and append
        # Simple append: add line under the last synonym
        import re as _re
        # Match the concept block and append the new synonym
        pattern = rf"(^{re.escape(concept)}:\n(?:[ \t]+.*\n)*)"
        def _append_syn(m):
            block = m.group(1).rstrip("\n")
            return block + f"\n    - \"{phrase}\"\n"
        new_raw, n = _re.subn(pattern, _append_syn, raw, flags=_re.MULTILINE)
        if n == 0:
            print(f"Could not locate '{concept}' block in YAML. Edit manually.")
            sys.exit(1)

        with open(syn_path, "w", encoding="utf-8") as f:
            f.write(new_raw)

        # Invalidate synonym cache
        import core.router as _router_mod
        _router_mod._synonyms_data = None

        print(f"Added '{phrase}' to concept '{concept}'.")

        # Auto-run routing eval to check nothing broke
        print("\nRunning routing eval to verify...")
        from core.eval import run_routing_eval
        run_routing_eval(verbose=False)
        return

    if args.audit:
        from core.router import QueryRouter
        router = QueryRouter()

        log_file = cfg.work_dir / "logs" / "queries.log"
        if not log_file.exists():
            print(f"No queries.log found at {log_file}")
            print("Run some queries first, then audit.")
            return

        import json as _json
        from collections import Counter
        queries_seen: list[str] = []
        with open(log_file, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    entry = _json.loads(line)
                    q = entry.get("query", "")
                    if q:
                        queries_seen.append(q)
                except Exception:
                    # Plain text line
                    queries_seen.append(line.split("|")[0].strip())

        # Take last 200
        recent = queries_seen[-200:]
        no_match: Counter = Counter()
        for q in recent:
            expanded = router.expand_query(q)
            if not expanded.concept_tags:
                no_match[q] += 1

        print(f"\nSynonym audit — last {len(recent)} queries")
        print(f"No concept match: {len(no_match)} unique queries\n")
        if no_match:
            print("Queries with NO concept match (most frequent first):")
            for q, cnt in no_match.most_common(30):
                print(f"  {cnt:>3}×  {q!r}")
            print(f"\nAdd missing phrases to: {syn_path}")
        else:
            print("All recent queries matched at least one concept.")
        return

    # Default: show expansion for a test query
    if args.query:
        from core.router import QueryRouter
        router = QueryRouter()
        query = " ".join(args.query)
        expanded = router.expand_query(query)
        print(f"\nQuery:         {query!r}")
        print(f"Normalised:    {expanded.normalised!r}")
        print(f"Concept tags:  {expanded.concept_tags or ['(none)']}")
        print(f"Expanded terms ({len(expanded.expanded_terms)}): "
              f"{expanded.expanded_terms[:10]}{'...' if len(expanded.expanded_terms) > 10 else ''}")
    else:
        print("Usage:\n"
              "  python tiga.py synonyms --audit\n"
              "  python tiga.py synonyms --add \"phrase\" --concept <concept>\n"
              "  python tiga.py synonyms <query text>")


def cmd_aliases(args: argparse.Namespace) -> None:
    """Manage project name aliases."""
    from config import cfg
    from core.db import get_connection

    cfg.ensure_dirs()
    conn = get_connection(cfg.get_db_path())

    try:
        if args.rebuild:
            from core.infer import generate_aliases_for_all
            print("Rebuilding project aliases...")
            added = generate_aliases_for_all(conn)
            print(f"Generated {added} alias(es) for all projects.")
            return

        if args.add and args.project:
            from core.infer import add_alias
            add_alias(conn, args.project, args.alias_name, alias_type="manual")
            print(f"Added alias '{args.alias_name}' for project {args.project}")
            return

        if args.project:
            # Show aliases for a project
            rows = conn.execute(
                "SELECT alias, alias_type, source FROM project_aliases WHERE project_code = ?",
                (args.project,),
            ).fetchall()
            if not rows:
                print(f"No aliases for project {args.project}.")
            else:
                print(f"\nAliases for {args.project}:")
                for r in rows:
                    print(f"  {r['alias']:<30} [{r['alias_type']}] from {r['source']}")
            return

        if args.audit:
            # Show unmatched files log
            log_file = cfg.work_dir / "logs" / "unmatched_files.log"
            if not log_file.exists():
                print("No unmatched files log yet.")
                return
            print(f"\nUnmatched files from {log_file}:")
            with open(log_file, encoding="utf-8") as f:
                for i, line in enumerate(f, 1):
                    print(f"  {i:4d}. {line.rstrip()}")
            return

        # Default: list all aliases
        rows = conn.execute(
            "SELECT project_code, alias, alias_type FROM project_aliases ORDER BY project_code, alias"
        ).fetchall()
        if not rows:
            print("No aliases yet. Run: python tiga.py aliases --rebuild")
        else:
            print(f"\n{'Project':<12} {'Alias':<35} {'Type'}")
            print("-" * 60)
            for r in rows:
                print(f"  {r['project_code']:<12} {r['alias']:<35} {r['alias_type']}")
    finally:
        conn.close()


def cmd_conventions(args: argparse.Namespace) -> None:
    """Detect, show, or manually override project folder conventions."""
    from config import cfg
    from core.db import get_connection
    from core.convention_detector import (
        detect_and_save, load_conventions, save_conventions, _name_to_canonical,
    )

    cfg.ensure_dirs()
    work_dir = cfg.work_dir

    # --unmapped: list files with no canonical_category
    if getattr(args, "unmapped", False):
        conn = get_connection(cfg.get_db_path())
        try:
            rows = conn.execute(
                "SELECT rel_path, project_id, folder_stage "
                "FROM files WHERE canonical_category IS NULL "
                "AND status != 'DISCOVERED' "
                "ORDER BY project_id, rel_path LIMIT 500"
            ).fetchall()
        finally:
            conn.close()

        if not rows:
            print("All indexed files have a canonical_category assigned.")
            return

        print(f"\nFiles with no canonical_category ({len(rows)} shown, max 500):\n")
        print(f"  {'Project':<16} {'Stage':<14} {'Path'}")
        print("  " + "-" * 80)
        for r in rows:
            print(
                f"  {str(r['project_id'] or ''):<16} "
                f"{str(r['folder_stage'] or ''):<14} "
                f"{r['rel_path']}"
            )
        return

    # --map: manually assign a folder name to a category
    if getattr(args, "map_folder", False):
        if not args.project or not args.folder or not args.category:
            print("Usage: python tiga.py conventions --map --project CODE "
                  "--folder \"Folder Name\" --category canonical_cat")
            sys.exit(1)

        data = load_conventions(work_dir)
        entry = data.setdefault(args.project, {})
        fm = entry.setdefault("folder_map", {})
        fm[args.folder] = args.category
        entry["folder_map"] = fm
        data[args.project] = entry
        save_conventions(work_dir, data)
        print(f"Mapped: {args.project} / '{args.folder}' -> '{args.category}'")
        return

    # --show: print saved convention for a project (or all)
    if getattr(args, "show", False):
        data = load_conventions(work_dir)
        if not data:
            print("No conventions saved yet. Run --detect first.")
            return

        if args.project:
            codes = [args.project] if args.project in data else []
            if not codes:
                print(f"No convention saved for project '{args.project}'.")
                return
        else:
            codes = sorted(data.keys())

        for code in codes:
            entry = data[code]
            print(f"\n{code}:")
            print(f"  Convention : {entry.get('convention_type', '?')}")
            print(f"  Prefix     : {entry.get('project_prefix', '') or '(none)'}")
            print(f"  Confidence : {entry.get('confidence', 0.0):.0%}")
            print(f"  Detected   : {entry.get('detected_at', '?')}")
            fm = entry.get("folder_map", {})
            if fm:
                print(f"  Folder map ({len(fm)} entries):")
                for folder, cat in sorted(fm.items()):
                    print(f"    {folder!r:<40} -> {cat}")
            else:
                print("  Folder map : (empty)")
        return

    # --detect: run detection for one project or all
    if getattr(args, "detect", False):
        # Build list of (project_code, project_root) pairs to detect
        targets: list[tuple[str, str]] = []

        if getattr(args, "all_projects", False):
            # Infer from index_roots
            for root in cfg.index_roots:
                root_path = root if hasattr(root, "iterdir") else __import__("pathlib").Path(root)
                if not root_path.is_dir():
                    continue
                # Each immediate subdir is a project
                for subdir in sorted(root_path.iterdir()):
                    if subdir.is_dir():
                        targets.append((subdir.name, str(subdir)))
            if not targets:
                # index_roots ARE the project roots
                for root in cfg.index_roots:
                    root_path = __import__("pathlib").Path(root)
                    targets.append((root_path.name, str(root_path)))
        else:
            if not args.project:
                print("Specify --project CODE or --all to detect conventions.")
                sys.exit(1)
            # Find the project root
            project_root = None
            for root in cfg.index_roots:
                root_path = __import__("pathlib").Path(root)
                # Check if root itself matches
                if args.project.lower() in root_path.name.lower():
                    project_root = str(root_path)
                    break
                # Check immediate subdirs
                for subdir in (root_path.iterdir() if root_path.is_dir() else []):
                    if args.project.lower() in subdir.name.lower() and subdir.is_dir():
                        project_root = str(subdir)
                        break
                if project_root:
                    break

            if not project_root:
                print(f"Could not locate project '{args.project}' in index_roots.")
                sys.exit(1)
            targets.append((args.project, project_root))

        if not targets:
            print("No project directories found to detect.")
            return

        print(f"Detecting conventions for {len(targets)} project(s)…\n")
        for code, root in targets:
            conv = detect_and_save(code, root, work_dir)
            print(
                f"  {code:<30} {conv.convention_type:<18} "
                f"prefix={conv.project_prefix or '(none)':<8} "
                f"conf={conv.confidence:.0%}  "
                f"folders_mapped={len(conv.folder_map)}"
            )
        print(f"\nSaved to: {work_dir / 'project_conventions.yml'}")
        return

    # Default: show help
    print(
        "Usage:\n"
        "  python tiga.py conventions --detect --project 186\n"
        "  python tiga.py conventions --detect --all\n"
        "  python tiga.py conventions --show\n"
        "  python tiga.py conventions --show --project 186\n"
        "  python tiga.py conventions --map --project 186 "
        "--folder \"Project Images\" --category renders_3d\n"
        "  python tiga.py conventions --unmapped"
    )


def cmd_einstein(_args: argparse.Namespace) -> None:
    from config import cfg

    if not cfg.einstein_enable:
        print("Einstein is disabled. Set einstein.enable: true in config.yaml.")
        print("(Phase 2 — not yet implemented)")
        return
    print("Einstein: Phase 2 not yet implemented.")


def cmd_scan(args: argparse.Namespace) -> None:
    """Windirstat-style file type / size scan of a folder."""
    import json as _json
    from tools.scanner import scan_folder, scan_for_phases, render_text, render_phases_text

    from pathlib import Path
    target = Path(args.path)
    if not target.exists():
        print(f"[ERROR] Path does not exist: {target}", file=sys.stderr)
        sys.exit(1)

    if args.phases:
        result = scan_for_phases(target, depth=args.depth)
        if args.as_json:
            print(_json.dumps(result, indent=2))
        else:
            print(render_phases_text(result))
    else:
        scan = scan_folder(target, top_files=args.top)
        if args.as_json:
            print(_json.dumps(scan.to_dict(), indent=2))
        else:
            print(render_text(scan, top=args.top))


def cmd_scrape_woha(args: argparse.Namespace) -> None:
    """Crawl woha.net and seed project_cards with project metadata."""
    import logging
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

    from tools.woha_scraper import run_scraper

    stats = run_scraper(
        dry_run=args.dry_run,
        delay=args.delay,
        limit=args.limit,
        typology_filter=getattr(args, "typology", None),
        db_path=getattr(args, "db", None),
    )
    print(
        f"\nWOHA scrape complete: "
        f"discovered={stats['discovered']} "
        f"scraped={stats['scraped']} "
        f"upserted={stats['upserted']} "
        f"failed={stats['failed']}"
    )


def cmd_schedule(args: argparse.Namespace) -> None:
    """
    Manage the time-of-day resource scheduler.

    Without flags: show current mode and settings.
    --mode day|night: force a specific mode now.
    --daemon: run as a continuous background daemon.
    """
    import logging
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

    from core.scheduler import (
        apply_mode, run_daemon, status_summary, _renice_ollama,
    )

    if args.daemon:
        interval = getattr(args, "interval", 60)
        run_daemon(check_interval_s=interval)
        return

    if args.mode:
        scfg = apply_mode(args.mode)
        _renice_ollama(args.mode)
        print(f"Mode set to: {args.mode}")
        print(f"  {scfg.description}")
        return

    # Default: show status
    s = status_summary()
    print(f"Current mode : {s['mode']}  (source: {s['source']})")
    print(f"Description  : {s['description']}")
    print(f"embed_batch  : {s['embed_batch_size']}")
    print(f"workers      : {s['extract_workers']}")
    print(f"run_indexing : {s['run_indexing']}")
    print(f"mode_file    : {s['mode_file']}")


def cmd_health(_args: argparse.Namespace) -> None:
    from config import cfg, ollama_available
    from core.db import get_connection

    print(f"Ollama ({cfg.ollama_base_url}) … ", end="", flush=True)
    print("OK" if ollama_available(cfg.ollama_base_url) else "UNREACHABLE")

    print("SQLite DB … ", end="", flush=True)
    try:
        conn = get_connection(cfg.get_db_path())
        conn.execute("SELECT 1")
        conn.close()
        print("OK")
    except Exception as e:
        print(f"FAIL ({e})")

    print(f"Chat model:  {cfg.chat_model}")
    print(f"Embed model: {cfg.embed_model}")


# ---------------------------------------------------------------------------
# Argument parser
# ---------------------------------------------------------------------------

def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="tiga",
        description="TIGA Hunt — archive search CLI",
    )
    sub = parser.add_subparsers(dest="command", required=True)

    # init
    p_init = sub.add_parser("init", help="Create default config.yaml")
    p_init.add_argument("--force", action="store_true", help="Overwrite existing config")

    # discover
    p_disc = sub.add_parser("discover", help="List files that would be indexed")
    p_disc.add_argument("--list", action="store_true", help="Print all file paths")
    p_disc.add_argument("--project", help="Filter to a specific project code")
    p_disc.add_argument("--parsed", action="store_true",
                        help="Show parsed path metadata for each file")

    # extract
    p_ext = sub.add_parser("extract", help="Test extraction on a file")
    p_ext.add_argument("file", help="Path to file")

    # embed
    p_emb = sub.add_parser("embed", help="Test Ollama embedding")
    p_emb.add_argument("query", nargs="+", help="Text to embed")

    # index
    sub.add_parser("index", help="Incremental index of archive")

    # rebuild
    sub.add_parser("rebuild", help="Force full re-index")

    # query
    p_q = sub.add_parser("query", help="Search the archive")
    p_q.add_argument("query", nargs="+", help="Search query")
    p_q.add_argument("--top-k", type=int, default=5)

    # status
    sub.add_parser("status", help="Show index statistics")

    # eval
    p_ev = sub.add_parser("eval", help="Run search quality evaluation")
    p_ev.add_argument("--queries", nargs="*", help="Custom test queries")
    p_ev.add_argument("--top-k", type=int, default=5)
    p_ev.add_argument("--review-failures", action="store_true",
                      help="Review last 20 low-confidence queries interactively")
    p_ev.add_argument("--routing", action="store_true",
                      help="Run routing-only eval (fast, no Ollama required)")
    p_ev.add_argument("--stress", action="store_true",
                      help="Run all 100 stress-test questions end-to-end and export HTML report")
    p_ev.add_argument("--project", default=None,
                      help="Filter stress eval to a category or keyword (use with --stress)")

    # serve
    sub.add_parser("serve", help="Start FastAPI LAN server")

    # ui
    sub.add_parser("ui", help="Start Streamlit UI")

    # health
    sub.add_parser("health", help="Check Ollama + DB connectivity")

    # einstein
    sub.add_parser("einstein", help="Phase 2 (stub)")

    # card
    p_card = sub.add_parser("card", help="View or edit a project data card")
    p_card.add_argument("project_code", help="Project code (e.g. 186)")
    p_card.add_argument("--woha-url", dest="woha_url", default=None,
                        help="Scrape WOHA project page to pre-populate")
    p_card.add_argument("--show", action="store_true",
                        help="Print card without editing")

    # correct
    p_corr = sub.add_parser("correct", help="Save a manual field correction")
    p_corr.add_argument("--project", required=True, help="Project code")
    p_corr.add_argument("--field", required=True, help="Field name")
    p_corr.add_argument("--value", required=True, help="Correct value")

    # cards
    p_cards = sub.add_parser("cards", help="List all project cards")
    p_cards.add_argument("--missing", action="store_true",
                         help="Show only cards with missing required fields")

    # route
    p_route = sub.add_parser("route", help="Show query routing classification")
    p_route.add_argument("query", nargs="+", help="Query to classify")

    # semantics
    p_sem = sub.add_parser("semantics", help="Audit folder name matching")
    p_sem.add_argument("--audit", action="store_true", help="Run audit")
    p_sem.add_argument("--project", default=None, help="Limit to project code")
    p_sem.add_argument("--all", dest="all_projects", action="store_true",
                       help="Audit all indexed projects")

    # synonyms
    p_syn = sub.add_parser("synonyms", help="Audit or extend query synonym map")
    p_syn.add_argument("query", nargs="*", help="Test query for expansion preview")
    p_syn.add_argument("--audit", action="store_true",
                       help="Scan queries.log for gaps in synonym coverage")
    p_syn.add_argument("--add", default=None, metavar="PHRASE",
                       help="Quick-add a phrase to a concept")
    p_syn.add_argument("--concept", default=None, metavar="CONCEPT",
                       help="Concept to add the phrase to (use with --add)")

    # conventions
    p_conv = sub.add_parser("conventions", help="Detect/show/override project folder conventions")
    p_conv.add_argument("--detect", action="store_true",
                        help="Detect convention for a project (or all with --all)")
    p_conv.add_argument("--all", dest="all_projects", action="store_true",
                        help="Detect conventions for all projects in index_roots")
    p_conv.add_argument("--show", action="store_true",
                        help="Print saved convention data")
    p_conv.add_argument("--project", default=None, metavar="CODE",
                        help="Project code to detect/show/map")
    p_conv.add_argument("--map", dest="map_folder", action="store_true",
                        help="Manually assign a folder to a canonical category")
    p_conv.add_argument("--folder", default=None, metavar="FOLDER_NAME",
                        help="Folder name to map (use with --map)")
    p_conv.add_argument("--category", default=None, metavar="CATEGORY",
                        help="Canonical category to assign (use with --map)")
    p_conv.add_argument("--unmapped", action="store_true",
                        help="List indexed files with no canonical_category")

    # aliases
    p_al = sub.add_parser("aliases", help="Manage project name aliases")
    p_al.add_argument("--project", default=None, help="Project code to show/manage")
    p_al.add_argument("--add", action="store_true", help="Add a manual alias")
    p_al.add_argument("--alias-name", dest="alias_name", default=None,
                      help="Alias to add (use with --add --project)")
    p_al.add_argument("--audit", action="store_true",
                      help="Show unmatched files log")
    p_al.add_argument("--rebuild", action="store_true",
                      help="Re-generate inferred aliases for all projects")

    # scrape-woha
    p_sw = sub.add_parser(
        "scrape-woha",
        help="Crawl woha.net and seed project_cards with project metadata",
    )
    p_sw.add_argument(
        "--dry-run", action="store_true",
        help="Print results without writing to DB",
    )
    p_sw.add_argument(
        "--delay", type=float, default=1.5,
        help="Seconds between HTTP requests (default 1.5)",
    )
    p_sw.add_argument(
        "--limit", type=int, default=0,
        help="Stop after N projects (0 = no limit)",
    )
    p_sw.add_argument(
        "--typology", default=None,
        help="Only scrape one typology page slug (e.g. 'hospitality')",
    )
    p_sw.add_argument(
        "--db", default=None,
        help="Override path to tiga.db",
    )

    # scan
    p_scan = sub.add_parser(
        "scan",
        help="Windirstat-style file type / size scan of a folder",
    )
    p_scan.add_argument("path", help="Folder to scan")
    p_scan.add_argument(
        "--phases", action="store_true",
        help="Scan sub-folders as separate projects and recommend test phases",
    )
    p_scan.add_argument(
        "--depth", type=int, default=1,
        help="Sub-folder depth for --phases mode (default 1)",
    )
    p_scan.add_argument(
        "--json", dest="as_json", action="store_true",
        help="Output machine-readable JSON",
    )
    p_scan.add_argument(
        "--top", type=int, default=20,
        help="Show top-N file types (default 20)",
    )

    # schedule
    p_sched = sub.add_parser(
        "schedule",
        help="Manage the time-of-day resource scheduler (day/night mode)",
    )
    p_sched.add_argument(
        "--mode", choices=["day", "night"], default=None,
        help="Force a specific mode now (day or night)",
    )
    p_sched.add_argument(
        "--daemon", action="store_true",
        help="Run as a continuous daemon that switches mode at day/night boundary",
    )
    p_sched.add_argument(
        "--interval", type=int, default=60,
        help="Daemon check interval in seconds (default 60)",
    )

    return parser


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main() -> None:
    parser = build_parser()
    args = parser.parse_args()

    dispatch = {
        "init":      cmd_init,
        "discover":  cmd_discover,
        "extract":   cmd_extract,
        "embed":     cmd_embed,
        "index":     cmd_index,
        "rebuild":   cmd_rebuild,
        "query":     cmd_query,
        "status":    cmd_status,
        "eval":      cmd_eval,
        "serve":     cmd_serve,
        "ui":        cmd_ui,
        "health":    cmd_health,
        "einstein":  cmd_einstein,
        "card":      cmd_card,
        "correct":   cmd_correct,
        "cards":     cmd_cards,
        "route":       cmd_route,
        "semantics":   cmd_semantics,
        "synonyms":    cmd_synonyms,
        "aliases":     cmd_aliases,
        "conventions": cmd_conventions,
        "scan":        cmd_scan,
        "scrape-woha": cmd_scrape_woha,
        "schedule":    cmd_schedule,
    }
    dispatch[args.command](args)


if __name__ == "__main__":
    main()
